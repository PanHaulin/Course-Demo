{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq-math.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F2FCghlvwIHX",
        "trQjbVQwsi3D"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujyDhwmqC3ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0eff90-0e03-4faf-8119-fdeca88e1a2d"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty\n",
        "\n",
        "import os\n",
        "os.chdir('drive/bert4keras')\n",
        "import sys\n",
        "sys.path.append('/root/.local/lib/python3.6/site-packages')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 145480 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.23-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.23-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vE9FlfWDfmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f3ef99-7bac-4902-97ab-83179568febe"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "!pip install bert4keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting bert4keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/bd/acb933644c7c205a487f2982e073a16553db3428b8fa903bd74151931000/bert4keras-0.9.7.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<=2.3.1 in /tensorflow-1.15.2/python3.6 (from bert4keras) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.19.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras<=2.3.1->bert4keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras<=2.3.1->bert4keras) (1.4.1)\n",
            "Building wheels for collected packages: bert4keras\n",
            "  Building wheel for bert4keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert4keras: filename=bert4keras-0.9.7-cp36-none-any.whl size=43299 sha256=f549508b9b16902f907d68bd821f951ad5f0d2d2c9dc51af6e48df82dfce3392\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/7b/06/4b4bca2005cfccd3a157cb012d1f91a83c252442c9358c238c\n",
            "Successfully built bert4keras\n",
            "Installing collected packages: bert4keras\n",
            "Successfully installed bert4keras-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKuVgsrFEkso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a6567f-35ba-4e85-a5a5-63bdcd424d89"
      },
      "source": [
        "from __future__ import division\n",
        "import json, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.layers import Loss\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.tokenizers import Tokenizer, load_vocab\n",
        "from bert4keras.optimizers import Adam\n",
        "from bert4keras.snippets import sequence_padding, open\n",
        "from bert4keras.snippets import DataGenerator, AutoRegressiveDecoder\n",
        "from keras.models import Model\n",
        "from sympy import Integer\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB6gehUBe2IO"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu1ER5vje4ba"
      },
      "source": [
        "# 基本参数\r\n",
        "maxlen = 192\r\n",
        "batch_size = 32\r\n",
        "epochs = 25\r\n",
        "\r\n",
        "# bert base配置\r\n",
        "config_path = 'uer/mixed_corpus_bert_base_model/bert_config.json'\r\n",
        "checkpoint_path = 'uer/mixed_corpus_bert_base_model/bert_model.ckpt'\r\n",
        "dict_path = 'uer/mixed_corpus_bert_base_model/vocab.txt'\r\n",
        "weights_path = 'ape210k/bestmodel/best_model.weights'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ehjt8Gv_wS"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_p9Kbl3gdYK"
      },
      "source": [
        "def is_equal(a, b):\r\n",
        "  a = round(float(a), 6)\r\n",
        "  b = round(float(b), 6)\r\n",
        "  return a == b\r\n",
        "\r\n",
        "\r\n",
        "def remove_bucket(equation):\r\n",
        "  l_buckets, buckets = [], []\r\n",
        "  for i, c in enumerate(equation):\r\n",
        "    if c == '(':\r\n",
        "      l_buckets.append(i)\r\n",
        "    elif c == ')':\r\n",
        "      buckets.append((l_buckets.pop(), i))\r\n",
        "  eval_equation = eval(equation)\r\n",
        "  for l, r in buckets:\r\n",
        "    new_equation = '%s %s %s' % (\r\n",
        "      equation[:l], equation[l + 1:r], equation[r + 1:]\r\n",
        "    )\r\n",
        "    try:\r\n",
        "      if is_equal(eval(new_equation.replace(' ', '')), eval_equation):\r\n",
        "        equation = new_equation\r\n",
        "    except:\r\n",
        "      pass\r\n",
        "  return equation.replace(' ', '')\r\n",
        "  \r\n",
        "class data_generator(DataGenerator):\r\n",
        "\r\n",
        "  def __iter__(self, random=False):\r\n",
        "    batch_token_ids, batch_segment_ids = [], []\r\n",
        "    for is_end, (question, equation, answer) in self.sample(random):\r\n",
        "      token_ids, segment_ids = tokenizer.encode(\r\n",
        "        question, equation, maxlen=maxlen\r\n",
        "      )\r\n",
        "      batch_token_ids.append(token_ids)\r\n",
        "      batch_segment_ids.append(segment_ids)\r\n",
        "      if len(batch_token_ids) == self.batch_size or is_end:\r\n",
        "        batch_token_ids = sequence_padding(batch_token_ids)\r\n",
        "        batch_segment_ids = sequence_padding(batch_segment_ids)\r\n",
        "        yield [batch_token_ids, batch_segment_ids], None\r\n",
        "        batch_token_ids, batch_segment_ids = [], []\r\n",
        "\r\n",
        "\r\n",
        "class CrossEntropy(Loss):\r\n",
        "  def compute_loss(self, inputs, mask=None):\r\n",
        "    y_true, y_mask, y_pred = inputs\r\n",
        "    y_true = y_true[:, 1:]  # 目标token_ids\r\n",
        "    y_mask = y_mask[:, 1:]  # segment_ids，刚好指示了要预测的部分\r\n",
        "    y_pred = y_pred[:, :-1]  # 预测序列，错开一位\r\n",
        "    loss = K.sparse_categorical_crossentropy(y_true, y_pred)\r\n",
        "    loss = K.sum(loss * y_mask) / K.sum(y_mask)\r\n",
        "    return loss\r\n",
        "\r\n",
        "class AutoSolve(AutoRegressiveDecoder):\r\n",
        "  \"\"\"\r\n",
        "  使用Bert+UNILM\r\n",
        "  \"\"\"\r\n",
        "  @AutoRegressiveDecoder.wraps(default_rtype='probas')\r\n",
        "  def predict(self, inputs, output_ids, states):\r\n",
        "    token_ids, segment_ids = inputs\r\n",
        "    token_ids = np.concatenate([token_ids, output_ids], 1)\r\n",
        "    segment_ids = np.concatenate([segment_ids, np.ones_like(output_ids)], 1)\r\n",
        "    return model.predict([token_ids, segment_ids])[:, -1]\r\n",
        "\r\n",
        "  def generate(self, text, topk=1):\r\n",
        "    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\r\n",
        "    output_ids = self.beam_search([token_ids, segment_ids], topk)  # 基于beam search\r\n",
        "    return tokenizer.decode(output_ids).replace(' ', '')\r\n",
        "\r\n",
        "class Evaluator(keras.callbacks.Callback):\r\n",
        "  def __init__(self):\r\n",
        "    self.best_acc = 0.\r\n",
        "\r\n",
        "  def on_epoch_end(self, epoch, logs=None):\r\n",
        "    metrics = self.evaluate(valid_data)  # 评测模型\r\n",
        "    if metrics['acc'] >= self.best_acc:\r\n",
        "      self.best_acc = metrics['acc']\r\n",
        "      model.save_weights('best_model.weights')  # 保存模型\r\n",
        "    metrics['best_acc'] = self.best_acc\r\n",
        "    print('valid_data:', metrics)\r\n",
        "\r\n",
        "  def evaluate(self, data, topk=1):\r\n",
        "    total, right = 0.0, 0.0\r\n",
        "    for question, equation, answer in tqdm(data):\r\n",
        "      total += 1\r\n",
        "      pred_equation = autosolve.generate(question, topk)\r\n",
        "      try:\r\n",
        "        right += int(is_equal(eval(pred_equation), eval(answer)))\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "    return {'acc': right / total}\r\n",
        "\r\n",
        "# prepare data\r\n",
        "def load_data(filename):\r\n",
        "  D = []\r\n",
        "  for l in open(filename):\r\n",
        "    l = json.loads(l)\r\n",
        "    question, equation, answer = l['original_text'], l['equation'], l['ans']\r\n",
        "    # 处理带分数\r\n",
        "    question = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', question)\r\n",
        "    equation = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', equation)\r\n",
        "    answer = re.sub('(\\d+)\\((\\d+/\\d+)\\)', '(\\\\1+\\\\2)', answer)\r\n",
        "    equation = re.sub('(\\d+)\\(', '\\\\1+(', equation)\r\n",
        "    answer = re.sub('(\\d+)\\(', '\\\\1+(', answer)\r\n",
        "    # 分数去括号\r\n",
        "    question = re.sub('\\((\\d+/\\d+)\\)', '\\\\1', question)\r\n",
        "    # 处理百分数\r\n",
        "    equation = re.sub('([\\.\\d]+)%', '(\\\\1/100)', equation)\r\n",
        "    answer = re.sub('([\\.\\d]+)%', '(\\\\1/100)', answer)\r\n",
        "    # 冒号转除号、剩余百分号处理\r\n",
        "    equation = equation.replace(':', '/').replace('%', '/100')\r\n",
        "    answer = answer.replace(':', '/').replace('%', '/100')\r\n",
        "    if equation[:2] == 'x=':\r\n",
        "      equation = equation[2:]\r\n",
        "    try:\r\n",
        "      if is_equal(eval(equation), eval(answer)):\r\n",
        "        D.append((question, remove_bucket(equation), answer))\r\n",
        "    except:\r\n",
        "      continue\r\n",
        "  return D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2FCghlvwIHX"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZNnRntfABN"
      },
      "source": [
        "# load data\r\n",
        "train_data = load_data('ape210k/data/train.ape.json')\r\n",
        "valid_data = load_data('ape210k/data/valid.ape.json')\r\n",
        "test_data = load_data('ape210k/data/test.ape.json')\r\n",
        "\r\n",
        "evaluator = Evaluator()\r\n",
        "train_generator = data_generator(train_data, batch_size)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLn1r-YvjQfs",
        "outputId": "3699917d-c78b-43fa-ef44-9e1b54e53665"
      },
      "source": [
        "# 从头训练\r\n",
        "token_dict, keep_tokens = load_vocab(\r\n",
        "    dict_path=dict_path,\r\n",
        "    simplified=True,\r\n",
        "    startswith=['[PAD]', '[UNK]', '[CLS]', '[SEP]'],\r\n",
        ")\r\n",
        "tokenizer = Tokenizer(token_dict,do_lower_case=True) #编码器\r\n",
        "\r\n",
        "# 构建Bert\r\n",
        "model = build_transformer_model(\r\n",
        "  config_path,\r\n",
        "  checkpoint_path,\r\n",
        "  application='unilm',\r\n",
        "  keep_tokens=keep_tokens,  # 只保留keep_tokens中的字，精简原字表\r\n",
        ")\r\n",
        "\r\n",
        "output = CrossEntropy(2)(model.inputs + model.outputs)\r\n",
        "\r\n",
        "model = Model(model.inputs, output)\r\n",
        "model.compile(optimizer=Adam(2e-5))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "autosolve = AutoSolve(start_id=None, end_id=tokenizer._token_end_id, maxlen=64)\r\n",
        "\r\n",
        "# Train\r\n",
        "model.fit(\r\n",
        "    train_generator.forfit(),\r\n",
        "    steps_per_epoch=len(train_generator),\r\n",
        "    epochs=epochs,\r\n",
        "    callbacks=[evaluator]\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/1\n",
            "6263/6263 [==============================] - 6345s 1s/step - loss: 0.8136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4999/4999 [12:30<00:00,  6.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid_data: {'acc': 0.47989597919583915, 'best_acc': 0.47989597919583915}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f1e40067358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trQjbVQwsi3D"
      },
      "source": [
        "# Load and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJWEnIkasmA2",
        "outputId": "d918f943-f9d7-4e8f-a6a0-16e8ce5b4bfc"
      },
      "source": [
        "# load data\r\n",
        "train_data = load_data('ape210k/data/train.ape.json')\r\n",
        "valid_data = load_data('ape210k/data/valid.ape.json')\r\n",
        "test_data = load_data('ape210k/data/test.ape.json')\r\n",
        "\r\n",
        "# evaluator = Evaluator()\r\n",
        "\r\n",
        "#加载词表\r\n",
        "token_dict, keep_tokens = json.load(open('ape210k/bestmodel/token_dict_keep_tokens.json'))\r\n",
        "\r\n",
        "tokenizer = Tokenizer(token_dict,do_lower_case=True) \r\n",
        "\r\n",
        "# 构建Bert\r\n",
        "model = build_transformer_model(\r\n",
        "  config_path,\r\n",
        "  checkpoint_path,\r\n",
        "  application='unilm',\r\n",
        "  keep_tokens=keep_tokens,  \r\n",
        ")\r\n",
        "\r\n",
        "output = CrossEntropy(2)(model.inputs + model.outputs)\r\n",
        "\r\n",
        "model = Model(model.inputs, output)\r\n",
        "model.compile(optimizer=Adam(5e-5))\r\n",
        "model.summary()\r\n",
        "\r\n",
        "autosolve = AutoSolve(start_id=None, end_id=tokenizer._token_end_id, maxlen=64)\r\n",
        "\r\n",
        "#加载权重\r\n",
        "print('加载参数')\r\n",
        "model.load_weights(weights_path)\r\n",
        "print('加载完毕')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/keras/engine/training_utils.py:819: UserWarning: Output cross_entropy_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to cross_entropy_1.\n",
            "  'be expecting any data to be passed to {0}.'.format(name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     multiple             10433280    Input-Token[0][0]                \n",
            "                                                                 MLM-Norm[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Attention-UniLM-Mask (Lambda)   (None, 1, None, None 0           Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Attention-UniLM-Mask[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, None, 768)    590592      Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Bias (BiasAdd)              (None, None, 13585)  13585       Embedding-Token[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Activation (Activation)     (None, None, 13585)  0           MLM-Bias[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "cross_entropy_1 (CrossEntropy)  (None, None, 13585)  0           Input-Token[0][0]                \n",
            "                                                                 Input-Segment[0][0]              \n",
            "                                                                 MLM-Activation[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 96,489,745\n",
            "Trainable params: 96,489,745\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "加载参数\n",
            "加载完毕\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV9OzQ5IyfeZ"
      },
      "source": [
        "def predict(data, dataname,topk=1, print_num=5):\r\n",
        "  total, right = 0.0, 0.0\r\n",
        "  correct_print = 0\r\n",
        "  false_print = 0\r\n",
        "  for question, equation, answer in tqdm(data):\r\n",
        "    total += 1\r\n",
        "    pred_equation =autosolve.generate(question, topk)\r\n",
        "    try:\r\n",
        "      is_right = is_equal(eval(pred_equation), eval(answer))\r\n",
        "      right += int(is_right)\r\n",
        "      if correct_print < print_num:\r\n",
        "        if is_right:\r\n",
        "          correct_print += 1\r\n",
        "          print(' ')\r\n",
        "          print('> {}'.format(question))\r\n",
        "          print('= equation:{} \\t answer:{}'.format(equation, answer))\r\n",
        "          print('< euqation:{} \\t answer:{}'.format(pred_equation, eval(pred_equation)))\r\n",
        "          print(' ')\r\n",
        "      if false_print < print_num:\r\n",
        "        if not is_right:\r\n",
        "          false_print += 1\r\n",
        "          print(' ')\r\n",
        "          print('> {}'.format(question))\r\n",
        "          print('= equation:{} \\t answer:{}'.format(equation, answer))\r\n",
        "          print('< euqation:{} \\t answer:{}'.format(pred_equation, eval(pred_equation)))\r\n",
        "          print(' ')\r\n",
        "          \r\n",
        "    except:\r\n",
        "      pass\r\n",
        "  print('For {}, acc:{:.3f}'.format(dataname, right/total))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKq6mNcL9UwF",
        "outputId": "e2e539ac-d719-4bee-8367-52a4119fb32d"
      },
      "source": [
        "predict(train_data, 'train data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/200392 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 五年级同学参加义务捐书活动，五1班捐了500本，五2班捐的本数是五1班80%，五3班捐的本数是五2班120%，五1班和五3班比谁捐书多？(请用两种方法比较一下)．\n",
            "= equation:1 \t answer:1\n",
            "< euqation:1 \t answer:1\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/200392 [00:00<7:42:50,  7.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 小王要将150千克含药量20%的农药稀释成含药量5%的药水．需要加水多少千克？\n",
            "= equation:150*20/100/(5/100)-150 \t answer:450\n",
            "< euqation:150*20/100/(5/100)-150 \t answer:450.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/200392 [00:00<10:10:22,  5.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 一个圆形花坛的半径是4米，现在要扩建花坛，将半径增加1米，这时花坛的占地面积增加了多少米**2．\n",
            "= equation:3.14*(4+1)**2-3.14*4**2 \t answer:28.26\n",
            "< euqation:3.14*(4+1)**2-3.14*4**2 \t answer:28.259999999999998\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 4/200392 [00:00<10:32:13,  5.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 甲乙两数的差和商都是6．那么甲乙两数的和是多少？(请分别使用方程和列式方法解题)\n",
            "= equation:6/(6-1)*6+6/(6-1) \t answer:8.4\n",
            "< euqation:6/(6-1)*6+6/(6-1) \t answer:8.399999999999999\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 5/200392 [00:00<10:55:49,  5.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 一件西服原价3200元，六月份先降价1/8，又加价1/8，这件西服的现价是多少元？原价高还是现价高？\n",
            "= equation:3200*(1-1/8)*(1+1/8) \t answer:3150\n",
            "< euqation:3200*(1-1/8)*(1+1/8) \t answer:3150.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 8/200392 [00:01<10:33:13,  5.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 六1班原来男生占总数的2/5，又转来5名男生，现在男生占总数的5/11，女生有多少人？\n",
            "= equation:5/(5/11/(1-5/11)-2/5/(1-2/5)) \t answer:30\n",
            "< euqation:5/(5/11/(1-5/11)-2/5/(1-2/5)) \t answer:30.000000000000007\n",
            " \n",
            " \n",
            "> 一个超市购进5吨大米，5天卖出2000千克，还剩多少千克？\n",
            "= equation:5*1000-2000 \t answer:3000\n",
            "< euqation:5*1000-2000 \t answer:3000\n",
            " \n",
            " \n",
            "> 学校有75个篮球，35个排球，把这些球平均分给5个班，每个班分得几个球？\n",
            "= equation:(75+35)/5 \t answer:22\n",
            "< euqation:(75+35)/5 \t answer:22.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 10/200392 [00:01<8:20:35,  6.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 小红每分钟打110个字，她从10点开始打字，10点二十五分结束，共打了多少个字？\n",
            "= equation:110*25 \t answer:2750\n",
            "< euqation:110*25 \t answer:2750\n",
            " \n",
            " \n",
            "> 蔬菜市场运回茄子1200千克．运回的西红柿是茄子的1/3．西红柿有多少千克？\n",
            "= equation:1200*1/3 \t answer:400\n",
            "< euqation:1200*1/3 \t answer:400.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 196443/200392 [7:57:31<10:28,  6.29it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXuXd0h19YnI",
        "outputId": "1c86d1e8-ff42-4ec0-b259-40a27c0d9bad"
      },
      "source": [
        "predict(valid_data, 'valid_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4999 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/4999 [00:02<3:33:51,  2.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 学校把135本练习本平均分给3个班，每班多少本？\n",
            "= equation:135/3 \t answer:45\n",
            "< euqation:135/3 \t answer:45.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 3/4999 [00:02<1:52:41,  1.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 为了帮助四川地震灾民，工厂赶制一批救灾帐篷，第一车间完成了这批帐篷的1/5，第二车间完成了这批帐篷的1/4，还剩下这批帐篷的几分之几没完成？\n",
            "= equation:1-1/5-1/4 \t answer:(11/20)\n",
            "< euqation:1-1/5-1/4 \t answer:0.55\n",
            " \n",
            " \n",
            "> 有50个数的平均数是83，如果去掉其中两个数，这两个数的和是118，那么剩下的书的平均数是多少？\n",
            "= equation:(50*83-118)/(50-2) \t answer:84\n",
            "< euqation:(83*50-118)/(50-2) \t answer:84.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 6/4999 [00:03<59:39,  1.39it/s]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 种植一种观赏树木，死亡棵数与成活棵数的比是1：15，这种观赏树木的成活率是多少．\n",
            "= equation:15/(15+1)*100/100 \t answer:(93.75/100)\n",
            "< euqation:15/(1+15)*100/100 \t answer:0.9375\n",
            " \n",
            " \n",
            "> 我国首艘航母辽宁舰的弦号是16，这个数共有多少个因数．\n",
            "= equation:5 \t answer:5\n",
            "< euqation:16/4 \t answer:4.0\n",
            " \n",
            " \n",
            "> 一个圆锥的底面直径是4厘米，高3厘米，把它从顶点往下一刀切开，成为形状相同的两半，表面积增加多少平方厘米？\n",
            "= equation:4*3/2*2 \t answer:12\n",
            "< euqation:4*3/2*2 \t answer:12.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 9/4999 [00:03<34:36,  2.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 两个相同的数相乘，积是3600，这个数是多少．\n",
            "= equation:60 \t answer:60\n",
            "< euqation:3600/1 \t answer:3600.0\n",
            " \n",
            " \n",
            "> 甲乙两球从同一地点沿周长为980米圆周匀速滚动，隔35分钟遇一次，若甲的速度是乙的3倍，求甲球滚动一周的时间．\n",
            "= equation:980/(3*980/(35*2)) \t answer:(23+1/3)\n",
            "< euqation:980/(980/35*3-980/35) \t answer:17.5\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 12/4999 [00:03<22:47,  3.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 一块0.15公顷的稻田，按行距2分米，穴距15厘米，一共可插秧多少株？\n",
            "= equation:15*10000/(2*0.15*10) \t answer:50000\n",
            "< euqation:0.15*10000*2*15/100/(0.15*10000) \t answer:0.3\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 17/4999 [00:05<22:52,  3.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 有6个棱长分别是4厘米、5厘米、6厘米的相同的长方体，把它们的某些面染上红色，使得6个长方体中染有红色的面恰好分别是1个面、2个面、3个面、4个面、5个面和6个面．染色后把所有长方体分割成棱长为1厘米的小正方体，分割完毕后，恰有一面是红色的小正方体最多有多少个？\n",
            "= equation:6*5/1*1+(5*4*2+5*2*2)/1*1*3+5*2*5+3*2+3*2*2+3*4*2+2*4*2 \t answer:318\n",
            "< euqation:(5-2)*4/1*1+(5-2)*4/1*2+(4-2)*5/1*2 \t answer:56.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4999/4999 [11:05<00:00,  7.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For valid_data, acc:0.717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yBBScnu9aSc",
        "outputId": "7f90e817-0f46-4cd2-86a9-d4ace98904b3"
      },
      "source": [
        "predict(test_data, 'test_data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/4998 [00:00<18:51,  4.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 2.75-(1+5/6)+(3+1/4)-(2+1/6)．\n",
            "= equation:2.75-(1+5/6)+3+1/4-(2+1/6) \t answer:2\n",
            "< euqation:2.75-(1+5/6)+3+1/4-(2+1/6) \t answer:1.9999999999999996\n",
            " \n",
            " \n",
            "> 王艳家买了一台洗衣机和一台电冰箱，一共花了6000元，电冰箱的价钱是洗衣机的3/5，求洗衣机的价钱．\n",
            "= equation:6000/(1+3/5) \t answer:3750\n",
            "< euqation:6000/(1+3/5) \t answer:3750.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/4998 [00:00<12:47,  6.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 有5筐苹果的重量相等，如果从每筐中取出10kg，那么剩下的苹果相当于原来3筐的重量，原来每筐苹果重多少千克？\n",
            "= equation:10*5/(5-3) \t answer:25\n",
            "< euqation:5*10/(5-3) \t answer:25.0\n",
            " \n",
            " \n",
            "> 王阿姨每分钟打60字，她15分钟能打多少字．\n",
            "= equation:60*15 \t answer:900\n",
            "< euqation:60*15 \t answer:900\n",
            " \n",
            " \n",
            "> 甲数是42，乙数是甲数的3/7，乙数是多少．\n",
            "= equation:42*3/7 \t answer:18\n",
            "< euqation:42*3/7 \t answer:18.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 9/4998 [00:01<13:45,  6.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 有李树5棵，每棵产李子60.8千克，桃树8棵，每棵产桃子47.5千克，收获哪种水果比较重？比另一种重多少千克？\n",
            "= equation:47.5*8-60.8*5 \t answer:76\n",
            "< euqation:60.8*5-47.5*8 \t answer:-76.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 11/4998 [00:01<12:24,  6.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 甲、乙两地相距120千米，客车和货车同时从甲地出发驶向乙地，客车到达乙地后立即沿原路返回，在途中的丙地与货车相遇．之后，客车和货车继续前进，各自到达甲地和乙地后又马上折回，结果两车又恰好在丙地相遇．已知两车在出发后的2小时首次相遇，那么客车的速度是每小时多少千米？\n",
            "= equation:(120+120/3)/2 \t answer:80\n",
            "< euqation:120*2/2 \t answer:120.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 17/4998 [00:02<07:54, 10.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 一个长方体的长和宽都是4米，高是5米，如果底面积扩大5倍，要使体积不变，高应该是多少厘米．\n",
            "= equation:5/5*100 \t answer:100\n",
            "< euqation:5*4*100/10 \t answer:200.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 19/4998 [00:02<11:00,  7.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 设上题答数是a=90．某项修桥工程，甲队单独做a天完成，乙队单独做270天完成，现在两队合做，中间甲队共休息了14天，乙队共休息了40天(但两队不会同一天休息)．那么从开始到完工共用了多少天．\n",
            "= equation:(1-1/270*14-1/90*40)/(1/90+1/270)+14+40 \t answer:88\n",
            "< euqation:135/1.5+(135-14)/(1/5) \t answer:695.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 20/4998 [00:02<12:04,  6.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " \n",
            "> 甲、乙两车从相距180千米的A地去B地，甲车比乙车晚1.5小时出发，结果两车同时到达．甲、乙两车的速度比是4：3．甲车每小时多少千米？\n",
            "= equation:180*(1-3/4)/1.5/(3/4) \t answer:40\n",
            "< euqation:180*3/(4+3-4) \t answer:180.0\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4998/4998 [10:58<00:00,  7.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For test_data, acc:0.716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}